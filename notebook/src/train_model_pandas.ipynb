{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cac9b46-1caf-4901-aa1f-18440732f167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 08:11:34.773616: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-21 08:11:34.775891: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-21 08:11:34.788231: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-21 08:11:34.827567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737421894.893442    5348 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737421894.914470    5348 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-21 08:11:34.980497: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directories...\n",
      "Creating directory: ['kaggle/datasets', 'kaggle/splits']\n",
      "Success: mkdir -p ../kaggle/datasets\n",
      "Output:\n",
      "\n",
      "Creating directory: ['kaggle/datasets', 'kaggle/splits']\n",
      "Success: mkdir -p ../kaggle/splits\n",
      "Output:\n",
      "\n",
      "Downloading dataset from Kaggle...\n",
      "Success: kaggle datasets download -d robertvici/indonesia-top-ecommerce-unicorn-tweets -p ../kaggle/datasets\n",
      "Output:\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/dataeng/.kaggle/kaggle.json'\n",
      "Dataset URL: https://www.kaggle.com/datasets/robertvici/indonesia-top-ecommerce-unicorn-tweets\n",
      "License(s): copyright-authors\n",
      "Downloading indonesia-top-ecommerce-unicorn-tweets.zip to ../kaggle/datasets\n",
      "\n",
      "\n",
      "Unziping the downloaded dataset...\n",
      "Success: unzip -o ../kaggle/datasets/indonesia-top-ecommerce-unicorn-tweets.zip -d ../kaggle/datasets\n",
      "Output:\n",
      "Archive:  ../kaggle/datasets/indonesia-top-ecommerce-unicorn-tweets.zip\n",
      "  inflating: ../kaggle/datasets/ShopeeID.json  \n",
      "  inflating: ../kaggle/datasets/bliblidotcom.json  \n",
      "  inflating: ../kaggle/datasets/bukalapak.json  \n",
      "  inflating: ../kaggle/datasets/lazadaID.json  \n",
      "  inflating: ../kaggle/datasets/tokopedia.json  \n",
      "\n",
      "Remowing indonesia-top-ecommerce-unicorn-tweets.zip file...\n",
      "Success: rm ../kaggle/datasets/indonesia-top-ecommerce-unicorn-tweets.zip\n",
      "Output:\n",
      "\n",
      "Loading datasets with Pandas...\n",
      "Adding a new column to identify the company source...\n",
      "Merging datasets using concat (equivalent to union in Spark)...\n",
      "Cleaning tweet tect\n",
      "Creating  feature for engagement...\n",
      "Selecting relevant features...\n",
      "Splitting dataset into train, validate, and test\n",
      "Replacing null values with 0...\n",
      "Replacing negative values with 0...\n",
      "Tokenize and vectorize text...\n",
      "Converting texts to sequences...\n",
      "Pad the sequences to ensure uniform length...\n",
      "Define a simple Neural Network model...\n",
      "Max index in X_train: 211977\n",
      "Shape of X_train: (378826, 68)\n",
      "Compiling the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 08:14:14.807874: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/10\n",
      "\u001b[1m11839/11839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 16ms/step - loss: 603710.3750 - mae: 27.7081\n",
      "Epoch 2/10\n",
      "\u001b[1m11839/11839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 16ms/step - loss: 616781.5625 - mae: 32.2200\n",
      "Epoch 3/10\n",
      "\u001b[1m11839/11839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 16ms/step - loss: 695101.0625 - mae: 34.8891\n",
      "Epoch 4/10\n",
      "\u001b[1m11839/11839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 16ms/step - loss: 575732.6250 - mae: 28.6435\n",
      "Epoch 5/10\n",
      "\u001b[1m11839/11839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 16ms/step - loss: 658079.4375 - mae: 30.2743\n",
      "Epoch 6/10\n",
      "\u001b[1m11839/11839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 16ms/step - loss: 652285.6875 - mae: 30.9110\n",
      "Epoch 7/10\n",
      "\u001b[1m11839/11839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 16ms/step - loss: 537915.9375 - mae: 29.6280\n",
      "Epoch 8/10\n",
      "\u001b[1m11839/11839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 16ms/step - loss: 1069750.0000 - mae: 33.2042\n",
      "Epoch 9/10\n",
      "\u001b[1m11839/11839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 16ms/step - loss: 677972.3750 - mae: 30.6285\n",
      "Epoch 10/10\n",
      "\u001b[1m11839/11839\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 17ms/step - loss: 413937.0312 - mae: 30.5293\n",
      "Saving the model...\n",
      "Saving tokenizer for future use...\n",
      "INFO:tensorflow:Assets written to: ../saved_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saved_model/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '../saved_model/1'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 68), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  125337784097232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  125337784097424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  125337784098576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  125337784097616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  125337784099920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Final Model ==> saved_model/1\n",
      "Keras Model, Tokenizer and Final Model are saved successfully!\n",
      "Removing local dataset files...\n",
      "Success: rm -r ../kaggle\n",
      "Output:\n",
      "\n",
      "\n",
      "All tasks completed successfully!\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# Program Name : train_model_pandas.ipynb\n",
    "# Purpose : train a model\n",
    "# Kaggle Dataset Source : obertvici/indonesia-top-ecommerce-unicorn-tweets\n",
    "# Location of Dataset Loaded : Local File System\n",
    "# Data Processsing Tools: pandas\n",
    "###########################################################################\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "def run_command(command):\n",
    "    \"\"\"Utility function to run shell commands\"\"\"\n",
    "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "    if process.returncode == 0:\n",
    "        print(f\"Success: {command}\\nOutput:\\n{stdout.decode()}\")\n",
    "    else:\n",
    "        print(f\"Error: {command}\\nError Message:\\n{stderr.decode()}\")\n",
    "\n",
    "\n",
    "# Create directories\n",
    "print(\"Creating directories...\")\n",
    "dirs = [\n",
    "    \"kaggle/datasets\",\n",
    "    \"kaggle/splits\"\n",
    "]\n",
    "\n",
    "for dir in dirs:\n",
    "    print(f\"Creating directory: {dirs}\")\n",
    "    run_command(f\"mkdir -p ../{dir}\")\n",
    "\n",
    "# Download dataset from Kaggle\n",
    "print(\"Downloading dataset from Kaggle...\")\n",
    "kaggle_dataset_path = \"../kaggle/datasets\"\n",
    "dataset_name = \"indonesia-top-ecommerce-unicorn-tweets\"\n",
    "run_command(f\"kaggle datasets download -d robertvici/{dataset_name} -p {kaggle_dataset_path}\")\n",
    "\n",
    "# Unzip the downloaded dataset\n",
    "print(\"Unziping the downloaded dataset...\")\n",
    "zip_file_path = f\"{kaggle_dataset_path}/{dataset_name}.zip\"\n",
    "run_command(f\"unzip -o {zip_file_path} -d {kaggle_dataset_path}\")\n",
    "\n",
    "# Remove indonesia-top-ecommerce-unicorn-tweets.zip\n",
    "print(\"Remowing indonesia-top-ecommerce-unicorn-tweets.zip file...\")\n",
    "run_command(f\"rm {zip_file_path}\")\n",
    "\n",
    "# Load datasets with Pandas\n",
    "print(\"Loading datasets with Pandas...\")\n",
    "blibli_df = pd.read_json(f'{kaggle_dataset_path}/bliblidotcom.json', lines=True)\n",
    "bukalapak_df = pd.read_json(f'{kaggle_dataset_path}/bukalapak.json', lines=True)\n",
    "lazadaID_df = pd.read_json(f'{kaggle_dataset_path}/lazadaID.json', lines=True)\n",
    "shopeeID_df = pd.read_json(f'{kaggle_dataset_path}/ShopeeID.json', lines=True)\n",
    "tokopedia_df = pd.read_json(f'{kaggle_dataset_path}/tokopedia.json', lines=True)\n",
    "\n",
    "# Add a new column to identify the company source\n",
    "print(\"Adding a new column to identify the company source...\")\n",
    "blibli_df['source'] = 'blibli'\n",
    "bukalapak_df['source'] = 'bukalapak'\n",
    "lazadaID_df['source'] = 'lazadaID'\n",
    "shopeeID_df['source'] = 'shopeeID'\n",
    "tokopedia_df['source'] = 'tokopedia'\n",
    "\n",
    "# Merge datasets using concat (equivalent to union in Spark)\n",
    "print(\"Merging datasets using concat (equivalent to union in Spark)...\")\n",
    "merged_df = pd.concat([blibli_df, bukalapak_df, lazadaID_df, shopeeID_df, tokopedia_df], axis=0)\n",
    "\n",
    "# Clean tweet text\n",
    "print(\"Cleaning tweet tect\")\n",
    "def clean_text(text):\n",
    "    return text.lower().replace(\"#\", \"\").strip()\n",
    "\n",
    "merged_df['clean_tweet'] = merged_df['tweet'].apply(clean_text)\n",
    "\n",
    "# Create new feature for engagement\n",
    "print(\"Creating  feature for engagement...\")\n",
    "merged_df['engagement'] = merged_df['replies_count'] + merged_df['retweets_count'] + merged_df['likes_count']\n",
    "\n",
    "# Select relevant features\n",
    "print(\"Selecting relevant features...\")\n",
    "selected_data = merged_df[['clean_tweet', 'replies_count', 'retweets_count', 'likes_count', 'engagement', 'hashtags', 'source']]\n",
    "\n",
    "# Split dataset into train, validate, and test\n",
    "print(\"Splitting dataset into train, validate, and test\")\n",
    "splits_dataset_path = \"../kaggle/splits\"\n",
    "train_data = selected_data.sample(frac=0.7, random_state=42)\n",
    "remaining_data = selected_data.drop(train_data.index)\n",
    "validate_data = remaining_data.sample(frac=0.5, random_state=42)\n",
    "test_data = remaining_data.drop(validate_data.index)\n",
    "\n",
    "# Replace null values with 0\n",
    "print(\"Replacing null values with 0...\")\n",
    "merged_df.fillna({\"likes_count\": 0, \"replies_count\": 0, \"retweets_count\": 0}, inplace=True)\n",
    "\n",
    "# Replace negative values with 0\n",
    "print(\"Replacing negative values with 0...\")\n",
    "for col in [\"likes_count\", \"replies_count\", \"retweets_count\"]:\n",
    "    merged_df[col] = merged_df[col].apply(lambda x: max(0, x))\n",
    "\n",
    "# Tokenize and vectorize text\n",
    "print (\"Tokenize and vectorize text...\")\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data[\"clean_tweet\"].values)\n",
    "\n",
    "# Convert texts to sequences\n",
    "print(\"Converting texts to sequences...\")\n",
    "X_train = tokenizer.texts_to_sequences(train_data[\"clean_tweet\"].values)\n",
    "\n",
    "# Pad the sequences to ensure uniform length\n",
    "print(\"Pad the sequences to ensure uniform length...\")\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post')\n",
    "\n",
    "y_train = train_data[\"engagement\"].values\n",
    "\n",
    "# Define a simple Neural Network model\n",
    "print(\"Define a simple Neural Network model...\")\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "])\n",
    "\n",
    "# Check Mxx index in X-train\n",
    "print(\"Max index in X_train:\", X_train.max())\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "\n",
    "# Filter / set index maximum to 5000\n",
    "X_train[X_train >= 5000] = 0\n",
    "\n",
    "# Compile the model\n",
    "print(\"Compiling the model...\")\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Save the model\n",
    "print(\"Saving the model...\")\n",
    "model.save(\"../models/e-commerce-engagement-model.keras\")\n",
    "\n",
    "# Save tokenizer for future use\n",
    "print(\"Saving tokenizer for future use...\")\n",
    "with open('../models/tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "\n",
    "# Export to save model\n",
    "model.export(\"../saved_model/1\")\n",
    "print(\"Final Model ==> saved_model/1\")\n",
    "\n",
    "print(\"Keras Model, Tokenizer and Final Model are saved successfully!\")\n",
    "\n",
    "# Remove datasets on local file system\n",
    "print(\"Removing local dataset files...\")\n",
    "run_command(f\"rm -r ../kaggle\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"All tasks completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed512ff-cb2c-4834-b8b0-bce02e8d5d61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
