{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b7069d-35f3-42f5-815d-5de0692bf908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 16:53:59.915741: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-19 16:53:59.920287: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-19 16:53:59.933848: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-19 16:53:59.977163: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737280440.032150   70770 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737280440.046141   70770 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-19 16:54:00.092754: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping DFS...\n",
      "Success: stop-dfs.sh\n",
      "Output:\n",
      "Stopping namenodes on [0.0.0.0]\n",
      "Stopping datanodes\n",
      "Stopping secondary namenodes [dataeng-virtual-machine]\n",
      "\n",
      "Starting DFS...\n",
      "Success: start-dfs.sh\n",
      "Output:\n",
      "Starting namenodes on [0.0.0.0]\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [dataeng-virtual-machine]\n",
      "\n",
      "Checking Java processes...\n",
      "Success: jps\n",
      "Output:\n",
      "71409 NameNode\n",
      "71541 DataNode\n",
      "28055 SparkSubmit\n",
      "71978 Jps\n",
      "40620 SparkSubmit\n",
      "71756 SecondaryNameNode\n",
      "43775 SparkSubmit\n",
      "\n",
      "Checking Safe Mode status...\n",
      "Success: hdfs dfsadmin -safemode get\n",
      "Output:\n",
      "Safe mode is ON\n",
      "\n",
      "Leaving Safe Mode if necessary...\n",
      "Success: hdfs dfsadmin -safemode leave\n",
      "Output:\n",
      "Safe mode is OFF\n",
      "\n",
      "Creating HDFS directory: kaggle/datasets\n",
      "Success: hdfs dfs -mkdir -p kaggle/datasets\n",
      "Output:\n",
      "\n",
      "Creating HDFS directory: kaggle/splits\n",
      "Success: hdfs dfs -mkdir -p kaggle/splits\n",
      "Output:\n",
      "\n",
      "Downloading dataset from Kaggle...\n",
      "Success: kaggle datasets download -d robertvici/indonesia-top-ecommerce-unicorn-tweets -p ../kaggle/datasets\n",
      "Output:\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/dataeng/.kaggle/kaggle.json'\n",
      "Dataset URL: https://www.kaggle.com/datasets/robertvici/indonesia-top-ecommerce-unicorn-tweets\n",
      "License(s): copyright-authors\n",
      "Downloading indonesia-top-ecommerce-unicorn-tweets.zip to ../kaggle/datasets\n",
      "\n",
      "\n",
      "Unzipping dataset...\n",
      "Success: unzip -o ../kaggle/datasets/indonesia-top-ecommerce-unicorn-tweets.zip -d ../kaggle/datasets\n",
      "Output:\n",
      "Archive:  ../kaggle/datasets/indonesia-top-ecommerce-unicorn-tweets.zip\n",
      "  inflating: ../kaggle/datasets/ShopeeID.json  \n",
      "  inflating: ../kaggle/datasets/bliblidotcom.json  \n",
      "  inflating: ../kaggle/datasets/bukalapak.json  \n",
      "  inflating: ../kaggle/datasets/lazadaID.json  \n",
      "  inflating: ../kaggle/datasets/tokopedia.json  \n",
      "\n",
      "Uploading Datasets JSON files to HDFS...\n",
      "Error: hdfs dfs -put ../kaggle/datasets/*.json kaggle/datasets\n",
      "Error Message:\n",
      "put: `kaggle/datasets/ShopeeID.json': File exists\n",
      "put: `kaggle/datasets/bliblidotcom.json': File exists\n",
      "put: `kaggle/datasets/bukalapak.json': File exists\n",
      "put: `kaggle/datasets/lazadaID.json': File exists\n",
      "put: `kaggle/datasets/tokopedia.json': File exists\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/19 16:55:26 WARN Utils: Your hostname, dataeng-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.241.136 instead (on interface ens33)\n",
      "25/01/19 16:55:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/19 16:55:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/19 16:55:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/01/19 16:55:29 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/01/19 16:55:29 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/01/19 16:55:43 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train, validate, and test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/19 16:56:18 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "|cashtags|conversation_id|created_at|date|geo|hashtags| id|likes_count|link|mentions|name|near|photos|place|quote_url|replies_count|reply_to|retweet|retweet_date|retweet_id|retweets_count|source|time|timezone|trans_dest|trans_src|translate|tweet|urls|user_id|user_rt|user_rt_id|username|video|\n",
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 16:57:24.118847: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 14ms/step - loss: 860127.8125 - mae: 29.6059\n",
      "Epoch 2/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 14ms/step - loss: 818085.5000 - mae: 32.1735\n",
      "Epoch 3/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 15ms/step - loss: 720687.6250 - mae: 27.6550\n",
      "Epoch 4/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 14ms/step - loss: 688658.5625 - mae: 26.6733\n",
      "Epoch 5/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 14ms/step - loss: 553800.1875 - mae: 25.7240\n",
      "Epoch 6/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 14ms/step - loss: 456790.7500 - mae: 23.3270\n",
      "Epoch 7/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 14ms/step - loss: 561675.6250 - mae: 24.1938\n",
      "Epoch 8/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 14ms/step - loss: 653932.3750 - mae: 26.7132\n",
      "Epoch 9/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 14ms/step - loss: 486970.5938 - mae: 26.8105\n",
      "Epoch 10/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 14ms/step - loss: 1080349.5000 - mae: 32.7191\n",
      "INFO:tensorflow:Assets written to: ../saved_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saved_model/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '../saved_model/1'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 60), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  130643582587280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130643582587472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130643582588624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130643582586128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130643582589968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Final Model ==> saved_model/1\n",
      "\n",
      "All tasks completed successfully!\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# Program Name : train_model_pyspark.ipynb\n",
    "# Purpose : train a model\n",
    "# Kaggle Dataset Source : obertvici/indonesia-top-ecommerce-unicorn-tweets\n",
    "# Location of Dataset Loaded : Linux HDFS\n",
    "# Data Processsing Tools: pyspark\n",
    "###########################################################################\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, lower, regexp_replace, count\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import lit\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "def run_command(command):\n",
    "    \"\"\"Utility function to run shell commands\"\"\"\n",
    "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "    if process.returncode == 0:\n",
    "        print(f\"Success: {command}\\nOutput:\\n{stdout.decode()}\")\n",
    "    else:\n",
    "        print(f\"Error: {command}\\nError Message:\\n{stderr.decode()}\")\n",
    "\n",
    "# Restart DFS\n",
    "print(\"Stopping DFS...\")\n",
    "run_command(\"stop-dfs.sh\")\n",
    "\n",
    "print(\"Starting DFS...\")\n",
    "run_command(\"start-dfs.sh\")\n",
    "\n",
    "# Check running Java services\n",
    "print(\"Checking Java processes...\")\n",
    "run_command(\"jps\")\n",
    "\n",
    "# Check Safe Mode status\n",
    "print(\"Checking Safe Mode status...\")\n",
    "run_command(\"hdfs dfsadmin -safemode get\")\n",
    "\n",
    "# Leave Safe Mode if it's ON\n",
    "print(\"Leaving Safe Mode if necessary...\")\n",
    "run_command(\"hdfs dfsadmin -safemode leave\")\n",
    "\n",
    "# Create HDFS directories\n",
    "hdfs_dirs = [\n",
    "    \"kaggle/datasets\",\n",
    "    \"kaggle/splits\"\n",
    "]\n",
    "\n",
    "for hdfs_dir in hdfs_dirs:\n",
    "    print(f\"Creating HDFS directory: {hdfs_dir}\")\n",
    "    run_command(f\"hdfs dfs -mkdir -p {hdfs_dir}\")\n",
    "\n",
    "# Download dataset from Kaggle\n",
    "kaggle_dataset_path = \"../kaggle/datasets\"\n",
    "dataset_name = \"indonesia-top-ecommerce-unicorn-tweets\"\n",
    "print(\"Downloading dataset from Kaggle...\")\n",
    "run_command(f\"kaggle datasets download -d robertvici/{dataset_name} -p {kaggle_dataset_path}\")\n",
    "\n",
    "# Unzip the downloaded dataset\n",
    "zip_file_path = f\"{kaggle_dataset_path}/{dataset_name}.zip\"\n",
    "print(\"Unzipping dataset...\")\n",
    "run_command(f\"unzip -o {zip_file_path} -d {kaggle_dataset_path}\")\n",
    "\n",
    "# Upload files to HDFS\n",
    "print(\"Uploading Datasets JSON files to HDFS...\")\n",
    "run_command(f\"hdfs dfs -put {kaggle_dataset_path}/*.json kaggle/datasets\")\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"E-Commerce Engagement Prediction ML\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets from HDFS\n",
    "blibli_df = spark.read.json('hdfs://localhost:9000/user/dataeng/kaggle/datasets/bliblidotcom.json')\n",
    "bukalapak_df = spark.read.json('hdfs://localhost:9000/user/dataeng/kaggle/datasets/bukalapak.json')\n",
    "lazadaID_df = spark.read.json('hdfs://localhost:9000/user/dataeng/kaggle/datasets/lazadaID.json')\n",
    "shopeeID_df = spark.read.json('hdfs://localhost:9000/user/dataeng/kaggle/datasets/ShopeeID.json')\n",
    "tokopedia_df = spark.read.json('hdfs://localhost:9000/user/dataeng/kaggle/datasets/tokopedia.json')\n",
    "\n",
    "# Add a new column to identify the company source\n",
    "blibli_df = blibli_df.withColumn('source', lit('blibli'))\n",
    "bukalapak_df = bukalapak_df.withColumn('source', lit('bukalapak'))\n",
    "lazadaID_df = lazadaID_df.withColumn('source', lit('lazadaID'))\n",
    "shopeeID_df = shopeeID_df.withColumn('source', lit('shopeeID'))\n",
    "tokopedia_df = tokopedia_df.withColumn('source', lit('tokopedia'))\n",
    "\n",
    "# Merge datasets using union (axis=0 equivalent in Spark)\n",
    "merged_df = blibli_df.union(bukalapak_df).union(lazadaID_df).union(shopeeID_df).union(tokopedia_df)\n",
    "\n",
    "# Clean tweet text\n",
    "def clean_text(text):\n",
    "    return text.lower().replace(\"#\", \"\").strip()\n",
    "\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "# Apply text cleaning and create new features\n",
    "data_cleaned = merged_df.withColumn(\"clean_tweet\", clean_text_udf(col(\"tweet\"))) \\\n",
    "                       .withColumn(\"engagement\", col(\"replies_count\") + col(\"retweets_count\") + col(\"likes_count\"))\n",
    "\n",
    "# Select relevant features\n",
    "selected_data = data_cleaned.select(\n",
    "    col(\"clean_tweet\").alias(\"text\"),\n",
    "    col(\"replies_count\").alias(\"replies\"),\n",
    "    col(\"retweets_count\").alias(\"retweets\"),\n",
    "    col(\"likes_count\").alias(\"likes\"),\n",
    "    col(\"engagement\").alias(\"target\"),\n",
    "    col(\"hashtags\"),    \n",
    "    col(\"source\")\n",
    ")\n",
    "\n",
    "# Split dataset\n",
    "print(\"Splitting dataset into train, validate, and test\")\n",
    "train_data, validate_data, test_data = selected_data.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "# Save splits on HDFS for later use\n",
    "splits_dataset_path = \"../kaggle/splits\"\n",
    "train_data.write.json(f\"{splits_dataset_path}/train.json\", mode=\"overwrite\")\n",
    "validate_data.write.json(f\"{splits_dataset_path}/validate.json\", mode=\"overwrite\")\n",
    "test_data.write.json(f\"{splits_dataset_path}/test.json\", mode=\"overwrite\")\n",
    "\n",
    "# Remove datasets on local file system\n",
    "print(\"Removing local dataset files...\")\n",
    "run_command(f\"rm -r ../kaggle\")\n",
    "\n",
    "# Change null value with 0 (if any)\n",
    "merged_df = merged_df.fillna({\"likes_count\": 0, \"replies_count\": 0, \"retweets_count\": 0})\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Check negative value\n",
    "merged_df.filter((F.col(\"likes_count\") < 0) | (F.col(\"replies_count\") < 0) | (F.col(\"retweets_count\") < 0)).show()\n",
    "\n",
    "# Change negative value with 0 (if any)\n",
    "for col in [\"likes_count\", \"replies_count\", \"retweets_count\"]:\n",
    "    merged_df = merged_df.withColumn(col, F.when(F.col(col) < 0, 0).otherwise(F.col(col)))\n",
    "    \n",
    "# Matching target engagement definitions in Spark DataFrame\n",
    "blibli_df = blibli_df.withColumn(\"engagement\", F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "bukalapak_df = bukalapak_df.withColumn(\"engagement\", F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "lazadaID_df = lazadaID_df.withColumn(\"engagement\",   F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "shopeeID_df = shopeeID_df.withColumn(\"engagement\",   F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "tokopedia_df = tokopedia_df.withColumn(\"engagement\", F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "\n",
    "# Load train data (convert Spark DataFrame to Pandas)\n",
    "train_df = train_data.toPandas()\n",
    "\n",
    "# Tokenize and vectorize text (fit on original text, not the padded sequences)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df[\"text\"])  # Fit tokenizer on the raw text data\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train = tokenizer.texts_to_sequences(train_df[\"text\"])\n",
    "\n",
    "# Pad the sequences to ensure uniform length\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post')\n",
    "\n",
    "y_train = np.array(train_df[\"target\"])\n",
    "\n",
    "\n",
    "# Define a simple Neural Network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "vocab_size = 42500  # Customize with your tokenizer\n",
    "embedding_dim = 128\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "\n",
    "# Example: Tokenize and pad the input text\n",
    "max_vocab_size = 5000\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(train_df[\"text\"])  # 'train_df[\"text\"]' should be a list of strings\n",
    "\n",
    "# Example of saving the tokenizer\n",
    "with open('../models/tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(train_df[\"text\"])\n",
    "X_train = pad_sequences(X_train_sequences, padding='post')\n",
    "\n",
    "# Ensure y_train is in the correct format (e.g., a numpy array)\n",
    "y_train = np.array(train_df[\"target\"])  # Adjust this based on your target column\n",
    "\n",
    "# Train the model \n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Save the model with a valid file extension in local server\n",
    "model.save(\"../models/e-commerce-engagement-model.keras\")  # For the native Keras format\n",
    "\n",
    "# Export to save model\n",
    "model.export(\"../saved_model/1\")\n",
    "print(\"Final Model ==> saved_model/1\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "print(\"\")\n",
    "print(\"All tasks completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c5c7f-fba8-48b3-a561-90c7dac2d57e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
