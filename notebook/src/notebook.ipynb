{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "467ba864-47fd-4474-a5e8-6876f9cb6fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [0.0.0.0]\n",
      "Stopping datanodes\n",
      "Stopping secondary namenodes [dataeng-virtual-machine]\n",
      "Starting namenodes on [0.0.0.0]\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [dataeng-virtual-machine]\n"
     ]
    }
   ],
   "source": [
    "# Restart dfs\n",
    "!stop-dfs.sh\n",
    "!start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329ea11a-0272-48e0-bb3e-bbcd4c4d0087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72433 SparkSubmit\n",
      "126293 NameNode\n",
      "115524 SparkSubmit\n",
      "28055 SparkSubmit\n",
      "126614 SecondaryNameNode\n",
      "126838 Jps\n",
      "126394 DataNode\n"
     ]
    }
   ],
   "source": [
    "# Check java services running\n",
    "!jps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f58d9f7-4985-4fb6-b8be-b797b43de30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe mode is OFF\n"
     ]
    }
   ],
   "source": [
    "# Check Safe mode: should be OFF\n",
    "!hdfs dfsadmin -safemode get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "555bacff-8e91-436b-ad61-efa4f3c5488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe mode is OFF\n"
     ]
    }
   ],
   "source": [
    "# If Safe mode is ON --> execute hdfs dfsadmin -safemode leave\n",
    "!hdfs dfsadmin -safemode leave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "510b2629-66eb-43c8-b1bc-d349480adb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check hdfs home files\n",
    "!hdfs dfs -ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d43adfed-98fb-40ae-9445-d07ed0c69c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hdfs directories\n",
    "!hdfs dfs -mkdir -p \\\n",
    "    kaggle/datasets \\\n",
    "    kaggle/splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcbe113f-0f26-4604-b6ca-a72e98590548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - dataeng supergroup          0 2025-01-19 20:55 kaggle/datasets\n",
      "drwxr-xr-x   - dataeng supergroup          0 2025-01-19 20:55 kaggle/splits\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls kaggle/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "049f49ba-9f7d-47a2-b430-f9182a647ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/dataeng/.kaggle/kaggle.json'\n",
      "Dataset URL: https://www.kaggle.com/datasets/robertvici/indonesia-top-ecommerce-unicorn-tweets\n",
      "License(s): copyright-authors\n",
      "indonesia-top-ecommerce-unicorn-tweets.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Archive:  /home/dataeng/kaggle-datasets/indonesia-top-ecommerce-unicorn-tweets.zip\n",
      "  inflating: /home/dataeng/kaggle-datasets/ShopeeID.json  \n",
      "  inflating: /home/dataeng/kaggle-datasets/bliblidotcom.json  \n",
      "  inflating: /home/dataeng/kaggle-datasets/bukalapak.json  \n",
      "  inflating: /home/dataeng/kaggle-datasets/lazadaID.json  \n",
      "  inflating: /home/dataeng/kaggle-datasets/tokopedia.json  \n"
     ]
    }
   ],
   "source": [
    "# Download kaggle dataset into hdfs\n",
    "\n",
    "!kaggle datasets download -d robertvici/indonesia-top-ecommerce-unicorn-tweets -p ~/kaggle-datasets\n",
    "!unzip -o ~/kaggle-datasets/indonesia-top-ecommerce-unicorn-tweets.zip -d ~/kaggle-datasets\n",
    "!hdfs dfs -put ~/kaggle-datasets/*.json kaggle/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec040fbd-4aee-4764-90ba-03fcba4790ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "-rw-r--r--   3 dataeng supergroup  233134634 2025-01-19 20:55 /user/dataeng/kaggle/datasets/ShopeeID.json\n",
      "-rw-r--r--   3 dataeng supergroup   56417505 2025-01-19 20:55 /user/dataeng/kaggle/datasets/bliblidotcom.json\n",
      "-rw-r--r--   3 dataeng supergroup   46806779 2025-01-19 20:55 /user/dataeng/kaggle/datasets/bukalapak.json\n",
      "-rw-r--r--   3 dataeng supergroup  118395100 2025-01-19 20:55 /user/dataeng/kaggle/datasets/lazadaID.json\n",
      "-rw-r--r--   3 dataeng supergroup   29228846 2025-01-19 20:55 /user/dataeng/kaggle/datasets/tokopedia.json\n"
     ]
    }
   ],
   "source": [
    "# Show list of dataset files downloaded\n",
    "!hdfs dfs -ls /user/dataeng/kaggle/datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60b1de9a-ac0e-456c-91e1-102e8122efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 20:55:49.856440: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-19 20:55:49.858435: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-19 20:55:49.872684: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-19 20:55:49.914038: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737294949.984325  125677 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737294950.006968  125677 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-19 20:55:50.083576: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, lower, regexp_replace, count\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import lit\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc0eef93-3ec9-4e73-94f0-b13d4c0bf583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export SPARK_LOCAL_IP=\"192.168.241.136\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff51c0d7-24d7-4684-8405-d9b338a0e535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/19 20:56:05 WARN Utils: Your hostname, dataeng-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.241.136 instead (on interface ens33)\n",
      "25/01/19 20:56:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/19 20:56:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"E-Commerce Engagement Prediction ML\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b77c1433-07a5-443a-971a-0eba6569760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a979352-6ae1-4b4e-87d9-1387682172c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/19 20:56:20 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load datasets from HDFS\n",
    "\n",
    "blibli_df = spark.read.json('hdfs://localhost:9000/user/dataeng/kaggle/datasets/bliblidotcom.json')\n",
    "bukalapak_df = spark.read.json('hdfs://localhost:9000/user/dataeng/kaggle/datasets/bukalapak.json')\n",
    "lazadaID_df = spark.read.json('hdfs://localhost:9000/user/dataeng/kaggle/datasets/lazadaID.json')\n",
    "shopeeID_df = spark.read.json('hdfs://localhost:9000/user/dataeng/kaggle/datasets/ShopeeID.json')\n",
    "tokopedia_df = spark.read.json('hdfs://localhost:9000/user/dataeng/kaggle/datasets/tokopedia.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8354a34-42aa-450e-b4a1-34496b01efe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/19 20:56:37 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------+----------+---+--------------------+-------------------+-----------+--------------------+-----------------+----------+----+--------------------+-----+---------+-------------+--------------------+-------+------------+----------+--------------+------+--------+--------+----------+---------+---------+--------------------+--------------------+---------+-------+----------+------------+-----+\n",
      "|cashtags|    conversation_id|   created_at|      date|geo|            hashtags|                 id|likes_count|                link|         mentions|      name|near|              photos|place|quote_url|replies_count|            reply_to|retweet|retweet_date|retweet_id|retweets_count|source|    time|timezone|trans_dest|trans_src|translate|               tweet|                urls|  user_id|user_rt|user_rt_id|    username|video|\n",
      "+--------+-------------------+-------------+----------+---+--------------------+-------------------+-----------+--------------------+-----------------+----------+----+--------------------+-----+---------+-------------+--------------------+-------+------------+----------+--------------+------+--------+--------+----------+---------+---------+--------------------+--------------------+---------+-------+----------+------------+-----+\n",
      "|      []|1256116141040242689|1588316400000|2020-05-01|   | [#ramadanlebihbaik]|1256116141040242689|          1|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             0|blibli|17:00:00|    AEST|          |         |         |Sudah kepikiran m...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1256101043999436802|1588312800000|2020-05-01|   | [#ramadanlebihbaik]|1256101043999436802|          0|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             0|blibli|16:00:00|    AEST|          |         |         |Belanja pake kart...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1256085943099981825|1588309200000|2020-05-01|   |[#ramadan, #ramad...|1256085943099981825|          1|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             0|blibli|15:00:00|    AEST|          |         |         |Udah seminggu #Ra...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1256082063796498432|1588308276000|2020-05-01|   |                  []|1256082069408505856|          1|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             1|blibli|14:44:36|    AEST|          |         |         |Catet tanggal mai...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1256082063796498432|1588308276000|2020-05-01|   |    [#ngobrolbareng]|1256082066355007488|          1|https://twitter.c...|               []|Blibli.com|    |[https://pbs.twim...|     |         |            1|[{280834900, blib...|  false|            |          |             1|blibli|14:44:36|    AEST|          |         |         |Yuk! Gabung di #N...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1256082063796498432|1588308275000|2020-05-01|   |                  []|1256082063796498432|          1|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            1|[{280834900, blib...|  false|            |          |             2|blibli|14:44:35|    AEST|          |         |         |Udah bosen banget...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1256065886688931841|1588304418000|2020-05-01|   |       [#dirumahaja]|1256065886688931841|          3|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            1|[{280834900, blib...|  false|            |          |             1|blibli|13:40:18|    AEST|          |         |         |Long weekend! Bak...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1256060570828062720|1588303151000|2020-05-01|   |       [#mayday2020]|1256060570828062720|          3|https://twitter.c...|               []|Blibli.com|    |[https://pbs.twim...|     |         |            0|[{280834900, blib...|  false|            |          |             0|blibli|13:19:11|    AEST|          |         |         |Selamat Hari Buru...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1255509152165593088|1588302916000|2020-05-01|   |                  []|1256059585124659200|          0|https://twitter.c...|[surabayaheroes7]|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             0|blibli|13:15:16|    AEST|          |         |         |Cek di sini, ya h...|[https://www.blib...|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1255867000049373185|1588257000000|2020-05-01|   |[#blogbliblifriends]|1255867000049373185|          2|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             1|blibli|00:30:00|    AEST|          |         |         |Buang semua racun...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1255859449970876417|1588255200000|2020-05-01|   |[#blogbliblifriends]|1255859449970876417|          3|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             1|blibli|00:00:00|    AEST|          |         |         |Kalau aku;\\n\\nKer...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1255851902069346306|1588253400000|2020-04-30|   |[#ramadanlebihbai...|1255851902069346306|          1|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             0|blibli|23:30:00|    AEST|          |         |         |Biar #RamadanLebi...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1255844357120245762|1588251601000|2020-04-30|   | [#ramadanlebihbaik]|1255844357120245762|          2|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             0|blibli|23:00:01|    AEST|          |         |         |Biar WFH jadi mak...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1255842587807629312|1588251180000|2020-04-30|   |[#dirumahaja, #bl...|1255842587807629312|          1|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             1|blibli|22:53:00|    AEST|          |         |         |Hayo, selama #dir...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1255839572656746496|1588250461000|2020-04-30|   |[#maskeruntukindo...|1255839572656746496|          1|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             1|blibli|22:41:01|    AEST|          |         |         |Terbaru! Printed ...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1252614755489067008|1588250006000|2020-04-30|   |                  []|1255837667138211840|          0|https://twitter.c...|[indahdesriliana]|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             0|blibli|22:33:26|    AEST|          |         |         |              😋😋😋|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1255829250713817094|1588248000000|2020-04-30|   |[#dirumahaja, #ra...|1255829250713817094|          3|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             0|blibli|22:00:00|    AEST|          |         |         |Buat kamu yang se...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1255821702891819009|1588246200000|2020-04-30|   |    [#karenakamuno1]|1255821702891819009|          0|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             0|blibli|21:30:00|    AEST|          |         |         |Mau belanja kebut...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "|      []|1255815014427971584|1588244606000|2020-04-30|   | [#ramadanlebihbaik]|1255815014427971584|          3|https://twitter.c...|   [dayatpiliang]|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             0|blibli|21:03:26|    AEST|          |         |         |Selamat berbuka p...|                  []|280834900|       |          |bliblidotcom|    1|\n",
      "|      []|1255814154188845056|1588244400000|2020-04-30|   | [#ramadanlebihbaik]|1255814154188845056|          2|https://twitter.c...|               []|Blibli.com|    |                  []|     |         |            0|[{280834900, blib...|  false|            |          |             0|blibli|21:00:00|    AEST|          |         |         |#RamadanLebihBaik...|                  []|280834900|       |          |bliblidotcom|    0|\n",
      "+--------+-------------------+-------------+----------+---+--------------------+-------------------+-----------+--------------------+-----------------+----------+----+--------------------+-----+---------+-------------+--------------------+-------+------------+----------+--------------+------+--------+--------+----------+---------+---------+--------------------+--------------------+---------+-------+----------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column to identify the company source\n",
    "blibli_df = blibli_df.withColumn('source', lit('blibli'))\n",
    "bukalapak_df = bukalapak_df.withColumn('source', lit('bukalapak'))\n",
    "lazadaID_df = lazadaID_df.withColumn('source', lit('lazadaID'))\n",
    "shopeeID_df = shopeeID_df.withColumn('source', lit('shopeeID'))\n",
    "tokopedia_df = tokopedia_df.withColumn('source', lit('tokopedia'))\n",
    "\n",
    "# Merge datasets using union (axis=0 equivalent in Spark)\n",
    "merged_df = blibli_df.union(bukalapak_df).union(lazadaID_df).union(shopeeID_df).union(tokopedia_df)\n",
    "\n",
    "# Show merged data\n",
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17c61c38-6632-4594-9c74-100224ed0bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the merged DataFrame:\n",
      "root\n",
      " |-- cashtags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- conversation_id: string (nullable = true)\n",
      " |-- created_at: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- hashtags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- likes_count: long (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- mentions: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- near: string (nullable = true)\n",
      " |-- photos: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- quote_url: string (nullable = true)\n",
      " |-- replies_count: long (nullable = true)\n",
      " |-- reply_to: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- user_id: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |-- retweet: boolean (nullable = true)\n",
      " |-- retweet_date: string (nullable = true)\n",
      " |-- retweet_id: string (nullable = true)\n",
      " |-- retweets_count: long (nullable = true)\n",
      " |-- source: string (nullable = false)\n",
      " |-- time: string (nullable = true)\n",
      " |-- timezone: string (nullable = true)\n",
      " |-- trans_dest: string (nullable = true)\n",
      " |-- trans_src: string (nullable = true)\n",
      " |-- translate: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- urls: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- user_rt: string (nullable = true)\n",
      " |-- user_rt_id: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- video: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame structure\n",
    "print(\"Schema of the merged DataFrame:\")\n",
    "merged_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4a05cab-6fa7-4545-86c4-5de54133d72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=======================================================> (38 + 1) / 39]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the merged DataFrame: 541180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Number of rows\n",
    "row_count = merged_df.count()\n",
    "print(f\"Number of rows in the merged DataFrame: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47f83160-3283-4c27-9827-105cd79cf95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+-----+------+--------------------+------+\n",
      "|                text|replies|retweets|likes|target|            hashtags|source|\n",
      "+--------------------+-------+--------+-----+------+--------------------+------+\n",
      "|sudah kepikiran m...|      0|       0|    1|     1| [#ramadanlebihbaik]|blibli|\n",
      "|belanja pake kart...|      0|       0|    0|     0| [#ramadanlebihbaik]|blibli|\n",
      "|udah seminggu ram...|      0|       0|    1|     1|[#ramadan, #ramad...|blibli|\n",
      "|catet tanggal mai...|      0|       1|    1|     2|                  []|blibli|\n",
      "|yuk! gabung di ng...|      1|       1|    1|     3|    [#ngobrolbareng]|blibli|\n",
      "|udah bosen banget...|      1|       2|    1|     4|                  []|blibli|\n",
      "|long weekend! bak...|      1|       1|    3|     5|       [#dirumahaja]|blibli|\n",
      "|selamat hari buru...|      0|       0|    3|     3|       [#mayday2020]|blibli|\n",
      "|cek di sini, ya h...|      0|       0|    0|     0|                  []|blibli|\n",
      "|buang semua racun...|      0|       1|    2|     3|[#blogbliblifriends]|blibli|\n",
      "|kalau aku;\\n\\nker...|      0|       1|    3|     4|[#blogbliblifriends]|blibli|\n",
      "|biar ramadanlebih...|      0|       0|    1|     1|[#ramadanlebihbai...|blibli|\n",
      "|biar wfh jadi mak...|      0|       0|    2|     2| [#ramadanlebihbaik]|blibli|\n",
      "|hayo, selama diru...|      0|       1|    1|     2|[#dirumahaja, #bl...|blibli|\n",
      "|terbaru! printed ...|      0|       1|    1|     2|[#maskeruntukindo...|blibli|\n",
      "|              😋😋😋|      0|       0|    0|     0|                  []|blibli|\n",
      "|buat kamu yang se...|      0|       0|    3|     3|[#dirumahaja, #ra...|blibli|\n",
      "|mau belanja kebut...|      0|       0|    0|     0|    [#karenakamuno1]|blibli|\n",
      "|selamat berbuka p...|      0|       0|    3|     3| [#ramadanlebihbaik]|blibli|\n",
      "|ramadanlebihbaik ...|      0|       0|    2|     2| [#ramadanlebihbaik]|blibli|\n",
      "+--------------------+-------+--------+-----+------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Clean tweet text\n",
    "def clean_text(text):\n",
    "    return text.lower().replace(\"#\", \"\").strip()\n",
    "\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "# Apply text cleaning and create new features\n",
    "data_cleaned = merged_df.withColumn(\"clean_tweet\", clean_text_udf(col(\"tweet\"))) \\\n",
    "                       .withColumn(\"engagement\", col(\"replies_count\") + col(\"retweets_count\") + col(\"likes_count\"))\n",
    "\n",
    "# Select relevant features\n",
    "selected_data = data_cleaned.select(\n",
    "    col(\"clean_tweet\").alias(\"text\"),\n",
    "    col(\"replies_count\").alias(\"replies\"),\n",
    "    col(\"retweets_count\").alias(\"retweets\"),\n",
    "    col(\"likes_count\").alias(\"likes\"),\n",
    "    col(\"engagement\").alias(\"target\"),\n",
    "    col(\"hashtags\"),    \n",
    "    col(\"source\")\n",
    ")\n",
    "\n",
    "# Show processed data\n",
    "selected_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0d7f6bb-82f9-4152-9972-d244aced7f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "train_data, validate_data, test_data = selected_data.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "# Save splits for later use\n",
    "train_data.write.json(\"kaggle/splits/train.json\", mode=\"overwrite\")\n",
    "validate_data.write.json(\"kaggle/splits/validate.json\", mode=\"overwrite\")\n",
    "test_data.write.json(\"kaggle/splits/test.json\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e29f860-9aa1-4628-b8ca-91f075b460fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:====================================>                    (9 + 5) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "|cashtags|conversation_id|created_at|date|geo|hashtags| id|likes_count|link|mentions|name|near|photos|place|quote_url|replies_count|reply_to|retweet|retweet_date|retweet_id|retweets_count|source|time|timezone|trans_dest|trans_src|translate|tweet|urls|user_id|user_rt|user_rt_id|username|video|\n",
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check null values\n",
    "\n",
    "merged_df.filter(\n",
    "    (merged_df.likes_count.isNull()) | \n",
    "    (merged_df.replies_count.isNull()) | \n",
    "    (merged_df.retweets_count.isNull())\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "914beadf-8663-40e4-a083-cece2aa5f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change null value with 0 (if any)\n",
    "merged_df = merged_df.fillna({\"likes_count\": 0, \"replies_count\": 0, \"retweets_count\": 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "faeef860-a909-4ce0-afba-c83ec9175ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:============================================>           (11 + 3) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "|cashtags|conversation_id|created_at|date|geo|hashtags| id|likes_count|link|mentions|name|near|photos|place|quote_url|replies_count|reply_to|retweet|retweet_date|retweet_id|retweets_count|source|time|timezone|trans_dest|trans_src|translate|tweet|urls|user_id|user_rt|user_rt_id|username|video|\n",
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "+--------+---------------+----------+----+---+--------+---+-----------+----+--------+----+----+------+-----+---------+-------------+--------+-------+------------+----------+--------------+------+----+--------+----------+---------+---------+-----+----+-------+-------+----------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Check negative value\n",
    "merged_df.filter((F.col(\"likes_count\") < 0) | (F.col(\"replies_count\") < 0) | (F.col(\"retweets_count\") < 0)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de9ee086-8cc5-4f6c-bc9e-0e3a58d451bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change negative value with 0 (if any)\n",
    "for col in [\"likes_count\", \"replies_count\", \"retweets_count\"]:\n",
    "    merged_df = merged_df.withColumn(col, F.when(F.col(col) < 0, 0).otherwise(F.col(col)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60dd39fc-37b0-47f2-9592-33e388b13bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching target engagement definitions in Spark DataFrame\n",
    "blibli_df = blibli_df.withColumn(\"engagement\", F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "bukalapak_df = bukalapak_df.withColumn(\"engagement\", F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "lazadaID_df = lazadaID_df.withColumn(\"engagement\",   F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "shopeeID_df = shopeeID_df.withColumn(\"engagement\",   F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n",
    "tokopedia_df = tokopedia_df.withColumn(\"engagement\", F.col(\"likes_count\") + F.col(\"replies_count\") + F.col(\"retweets_count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc44acd8-b16c-4031-9daa-592901231add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load train data (convert Spark DataFrame to Pandas)\n",
    "train_df = train_data.toPandas()\n",
    "\n",
    "# Tokenize and vectorize text (fit on original text, not the padded sequences)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df[\"text\"])  # Fit tokenizer on the raw text data\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train = tokenizer.texts_to_sequences(train_df[\"text\"])\n",
    "\n",
    "# Pad the sequences to ensure uniform length\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post')\n",
    "\n",
    "y_train = np.array(train_df[\"target\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "660eacea-5c60-4f8b-8631-c7ac0c4ae681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple Neural Network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a6c46e9-340d-4297-8a02-836a810ede82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 20:58:37.996048: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17d22e52-f469-4541-baf5-ba422592fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 42500  # Customize with your tokenizer\n",
    "embedding_dim = 128\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64a39659-e862-47ab-a4f7-d1df11637ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 17ms/step - loss: 965121.7500 - mae: 31.1688\n",
      "Epoch 2/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 17ms/step - loss: 691859.5625 - mae: 29.4899\n",
      "Epoch 3/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 17ms/step - loss: 717546.7500 - mae: 29.6016\n",
      "Epoch 4/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 17ms/step - loss: 503083.4375 - mae: 26.5202\n",
      "Epoch 5/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 17ms/step - loss: 1040871.4375 - mae: 32.5972\n",
      "Epoch 6/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 17ms/step - loss: 940018.0625 - mae: 31.2613\n",
      "Epoch 7/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 17ms/step - loss: 609515.5625 - mae: 28.4569\n",
      "Epoch 8/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 17ms/step - loss: 703785.1250 - mae: 31.8703\n",
      "Epoch 9/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 16ms/step - loss: 451232.2812 - mae: 28.5009\n",
      "Epoch 10/10\n",
      "\u001b[1m11833/11833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 17ms/step - loss: 522296.0938 - mae: 30.4058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x763caca07410>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Tokenize and pad the input text\n",
    "max_vocab_size = 5000\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(train_df[\"text\"])  # 'train_df[\"text\"]' should be a list of strings\n",
    "\n",
    "# Example of saving the tokenizer\n",
    "with open('../models/tokenizer_notebook.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(train_df[\"text\"])\n",
    "X_train = pad_sequences(X_train_sequences, padding='post')\n",
    "\n",
    "# Ensure y_train is in the correct format (e.g., a numpy array)\n",
    "y_train = np.array(train_df[\"target\"])  # Adjust this based on your target column\n",
    "\n",
    "# Train the model !!!\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f405028-7ce6-442f-a83b-ac159f661646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model with a valid file extension in local server\n",
    "model.save(\"../models/e-commerce-engagement_model_notebook.keras\")  # For the native Keras format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aaaa9962-5bb0-4c83-9527-9a955b9740fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saved_model_notebook/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saved_model_notebook/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '../saved_model_notebook/1'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 60), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  130002563162064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130002563162256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130002563163408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130002563162448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130002563164752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "# Export to save final model\n",
    "model.export(\"../saved_model_notebook/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40c0863b-12c0-4842-b200-b9ffb666c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
